server:
  port: 8080 # ??????

spring:
  ai:
    deepseek:
      api-key: sk-15a030d883d94782b62887b1d142cb2c
      base-url: https://api.deepseek.com # DeepSeek 的请求 URL, 可不填，默认值为 api.deepseek.com
      chat:
        options:
          model: deepseek-chat # 使用哪个模型
          temperature: 0.8 # 温度值
    ollama:
      base-url: http://localhost:11434 # Ollama 服务的访问地址, 11434 端口是 Ollama 默认的启动端口
#      chat:
#        options: # 模型参数
#          model: qwen3:14b # 指定 Ollama 使用的大模型名称，根据你实际安装的来，我运行的是 14b
#          temperature: 0.7 # 温度值

logging:
  level:
    org.springframework.ai.chat.client.advisor: debug
